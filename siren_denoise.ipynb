{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6Xn_o9-slEz"
      },
      "source": [
        "# Siren Denoising\n",
        "This is a colab notebook that implements unsupervised image denoising with SIREN with several variants. The code corresponds to the paper *Denoising Capacity of Implicit Image Representation*. This notebook is modified from the [official implementation](https://github.com/vsitzmann/siren)'s `explore_siren.ipynb` notebook. For questions or comments, please contact me at leozdong@stanford.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GIB7L2V46kM"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXB12zuq25bK"
      },
      "source": [
        "This defines the LPIPS metric we will use to measure perceptual similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380,
          "referenced_widgets": [
            "9eb4937ef05f433a94f46bfb059ed24e",
            "1790f43845a14dfa9512b99f37e312f6",
            "d44e810563bf4d578ebf7ed1c61b6756",
            "39e8c663df504e72983e019ef52f3d48",
            "608aebb89a6740248df9eedf53408ec2",
            "4de6eefdf4f344349560ae1073950e66",
            "1159f1a1153f4142bffe4cfff80f1c2f",
            "764f602d8da542dbb1d25a8df161651c",
            "e8479317fd1b47f3b8fd183f29d7139f",
            "fbacc54f0ca2431195492e9915131493",
            "46a77907f1f24cdf99af2be45ccdb0f3"
          ]
        },
        "id": "lGgHdmOVDgc5",
        "outputId": "a24ae7e2-a4ac-4803-e587-e56bc2f5f846"
      },
      "outputs": [],
      "source": [
        "!pip install lpips\n",
        "import lpips\n",
        "loss_fn_alex = lpips.LPIPS(net='alex')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZF3hVMoSbMa",
        "outputId": "67c0b147-2e21-432d-a9e0-08dd050134bb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjFEgLiZslE4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Resize, Compose, ToTensor, Normalize\n",
        "import numpy as np\n",
        "import skimage\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "from skimage.metrics import peak_signal_noise_ratio as compare_psnr\n",
        "\n",
        "def get_mgrid(sidelen, dim=2):\n",
        "    '''Generates a flattened grid of (x,y,...) coordinates in a range of -1 to 1.\n",
        "    sidelen: int\n",
        "    dim: int'''\n",
        "    tensors = tuple(dim * [torch.linspace(-1, 1, steps=sidelen)])\n",
        "    mgrid = torch.stack(torch.meshgrid(*tensors), dim=-1)\n",
        "    mgrid = mgrid.reshape(-1, dim)\n",
        "    return mgrid\n",
        "  \n",
        "def rgb_float2uint(rgb):\n",
        "  # `rgb` in range [0, 1]\n",
        "  return (np.clip(rgb, a_min=0, a_max=1) * 255).astype(np.uint8)\n",
        "\n",
        "def lpips_np(img_targ, img):\n",
        "  with torch.no_grad():\n",
        "    return loss_fn_alex(torch.tensor(img_targ).permute(2, 0, 1), torch.tensor(img).permute(2, 0, 1)).item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lguFwo-lslE6"
      },
      "source": [
        "Now, we code up the sine layer, which will be the basic building block of SIREN. This is a much more concise implementation than the one in the main code, as here, we aren't concerned with the baseline comparisons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8zM1kvqslE7"
      },
      "outputs": [],
      "source": [
        "class SineLayer(nn.Module):\n",
        "    # See paper sec. 3.2, final paragraph, and supplement Sec. 1.5 for discussion of omega_0.\n",
        "    \n",
        "    # If is_first=True, omega_0 is a frequency factor which simply multiplies the activations before the \n",
        "    # nonlinearity. Different signals may require different omega_0 in the first layer - this is a \n",
        "    # hyperparameter.\n",
        "    \n",
        "    # If is_first=False, then the weights will be divided by omega_0 so as to keep the magnitude of \n",
        "    # activations constant, but boost gradients to the weight matrix (see supplement Sec. 1.5)\n",
        "    \n",
        "    def __init__(self, in_features, out_features, bias=True,\n",
        "                 is_first=False, omega_0=30):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "        self.is_first = is_first\n",
        "        \n",
        "        self.in_features = in_features\n",
        "        self.linear = nn.Linear(in_features, out_features, bias=bias)\n",
        "        \n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        with torch.no_grad():\n",
        "            if self.is_first:\n",
        "                self.linear.weight.uniform_(-1 / self.in_features, \n",
        "                                             1 / self.in_features)      \n",
        "            else:\n",
        "                self.linear.weight.uniform_(-np.sqrt(6 / self.in_features) / self.omega_0, \n",
        "                                             np.sqrt(6 / self.in_features) / self.omega_0)\n",
        "        \n",
        "    def forward(self, input):\n",
        "        return torch.sin(self.omega_0 * self.linear(input))\n",
        "    \n",
        "    def forward_with_intermediate(self, input): \n",
        "        # For visualization of activation distributions\n",
        "        intermediate = self.omega_0 * self.linear(input)\n",
        "        return torch.sin(intermediate), intermediate\n",
        "    \n",
        "    \n",
        "class Siren(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, hidden_layers, out_features, outermost_linear=False, \n",
        "                 first_omega_0=30, hidden_omega_0=30.):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.net = []\n",
        "        self.net.append(SineLayer(in_features, hidden_features, \n",
        "                                  is_first=True, omega_0=first_omega_0))\n",
        "\n",
        "        for i in range(hidden_layers):\n",
        "            self.net.append(SineLayer(hidden_features, hidden_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "\n",
        "        if outermost_linear:\n",
        "            final_linear = nn.Linear(hidden_features, out_features)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                final_linear.weight.uniform_(-np.sqrt(6 / hidden_features) / hidden_omega_0, \n",
        "                                              np.sqrt(6 / hidden_features) / hidden_omega_0)\n",
        "                \n",
        "            self.net.append(final_linear)\n",
        "        else:\n",
        "            self.net.append(SineLayer(hidden_features, out_features, \n",
        "                                      is_first=False, omega_0=hidden_omega_0))\n",
        "        \n",
        "        self.net = nn.Sequential(*self.net)\n",
        "    \n",
        "    def forward(self, coords):\n",
        "        coords = coords.clone().detach().requires_grad_(True) # allows to take derivative w.r.t. input\n",
        "        output = self.net(coords)\n",
        "        return output, coords        \n",
        "\n",
        "    def forward_with_activations(self, coords, retain_grad=False):\n",
        "        '''Returns not only model output, but also intermediate activations.\n",
        "        Only used for visualizing activations later!'''\n",
        "        activations = OrderedDict()\n",
        "\n",
        "        activation_count = 0\n",
        "        x = coords.clone().detach().requires_grad_(True)\n",
        "        activations['input'] = x\n",
        "        for i, layer in enumerate(self.net):\n",
        "            if isinstance(layer, SineLayer):\n",
        "                x, intermed = layer.forward_with_intermediate(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    intermed.retain_grad()\n",
        "                    \n",
        "                activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = intermed\n",
        "                activation_count += 1\n",
        "            else: \n",
        "                x = layer(x)\n",
        "                \n",
        "                if retain_grad:\n",
        "                    x.retain_grad()\n",
        "                    \n",
        "            activations['_'.join((str(layer.__class__), \"%d\" % activation_count))] = x\n",
        "            activation_count += 1\n",
        "\n",
        "        return activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yn4e3CtvBg1Q"
      },
      "source": [
        "Finally, we define Siren with spline positional encoding. This following class definition is modified from https://github.com/microsoft/SplinePosEnc/blob/main/models.py."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYl3aGN4_STZ"
      },
      "outputs": [],
      "source": [
        "class PosProj(nn.Module):\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    super().__init__()\n",
        "    assert(out_dim > in_dim)\n",
        "    self.in_dim = in_dim\n",
        "    self.out_dim = out_dim\n",
        "    self.channel = out_dim - in_dim\n",
        "    self.register_buffer('proj', torch.Tensor(in_dim, self.channel))\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    with torch.no_grad():\n",
        "      proj = torch.randn_like(self.proj)\n",
        "      scale = self.in_dim ** 0.5 # TODO: use small scale\n",
        "      scale = torch.norm(proj, dim=0, keepdim=True) * scale\n",
        "      proj = proj / (1.0e-6 + scale)\n",
        "      self.proj.copy_(proj)\n",
        "\n",
        "  def forward(self, coords):\n",
        "    proj = torch.flatten(coords, end_dim=1).mm(self.proj)\n",
        "    proj = proj.view(list(coords.size())[:-1] + [self.channel])\n",
        "    output = torch.cat([coords, proj], dim=-1)\n",
        "    return output\n",
        "\n",
        "\n",
        "class OptPosEnc(nn.Module):\n",
        "  def __init__(self, in_features, code_num=256, code_channel=64):\n",
        "    super().__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = code_channel\n",
        "    self.code_num = code_num\n",
        "\n",
        "    code_size = [code_channel, in_features * code_num]\n",
        "    self.shape_code = nn.Parameter(torch.Tensor(*code_size))\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self,):\n",
        "    nn.init.xavier_uniform_(self.shape_code)\n",
        "\n",
        "  def forward(self, coords):\n",
        "    return self._forward(coords, self.shape_code)\n",
        "\n",
        "  def _forward(self, coords, shape_code):\n",
        "    pt_num, in_features = coords.size(1), coords.size(2)\n",
        "    assert in_features == self.in_features\n",
        "    code_num = shape_code.size(1) // in_features\n",
        "    mul = [[[[code_num * i] for i in range(in_features)]]] # [1, 1, D, 1]\n",
        "    mul = torch.tensor(mul, dtype=torch.int64, device=coords.device)\n",
        "    mask = torch.tensor([[[[0, 1]]]], dtype=torch.float32, device=coords.device)\n",
        "\n",
        "    coords = (coords + 1.0) * ((code_num - 1) / 2.0) # [-1, 1] -> [0, code_num-1]\n",
        "    corners = torch.floor(coords).detach()    # [1, N, D]\n",
        "    corners = corners.unsqueeze(-1) + mask    # [1, N, D, 2]\n",
        "    index = corners.to(torch.int64) + mul     # [1, N, D, 2]\n",
        "    coordsf = coords.unsqueeze(-1) - corners  # [1, N, D, 2], local coords [-1, 1]\n",
        "    weights = 1.0 - torch.abs(coordsf)        # (1, N, D, 2)\n",
        "\n",
        "    coords_code = torch.index_select(shape_code, 1, index.view(-1))\n",
        "    coords_code = coords_code.view(-1, pt_num, in_features, 2) # (C, N, D, 2)\n",
        "    output = torch.sum(coords_code * weights, dim=(-2, -1), keepdim=True)\n",
        "    output = output.squeeze(-1).permute(2, 1, 0)\n",
        "    return output\n",
        "\n",
        "  def upsample(self, size=64):\n",
        "    code = self.shape_code.view(self.out_features, self.in_features, self.code_num)\n",
        "    code = code.permute(1, 0, 2)\n",
        "    output = torch.nn.functional.upsample(code, size=size, mode='linear',\n",
        "                                          align_corners=True)\n",
        "    output = output.permute(1, 0, 2)\n",
        "    output = output.reshape(self.out_features, -1)\n",
        "    return output\n",
        "\n",
        "\n",
        "class PosEncSiren(nn.Module):\n",
        "  def __init__(self, in_features=2, out_features=3, hidden_layers=3,\n",
        "               hidden_features=256, projs=32):\n",
        "    super().__init__()\n",
        "    assert projs >= in_features\n",
        "    self.proj = PosProj(in_features, projs)\n",
        "    in_features = projs\n",
        "    self.pos_enc = OptPosEnc(in_features)\n",
        "    self.net = Siren(in_features=self.pos_enc.out_features, \n",
        "                     out_features=out_features, hidden_features=hidden_features, \n",
        "                     hidden_layers=hidden_layers, outermost_linear=True)\n",
        "    self.reset_parameters()\n",
        "\n",
        "  def reset_parameters(self):\n",
        "    with torch.no_grad():\n",
        "      shape_code = torch.zeros_like(self.pos_enc.shape_code)\n",
        "      channel, num = self.pos_enc.shape_code.shape\n",
        "      code_num, in_features = self.pos_enc.code_num, self.pos_enc.in_features\n",
        "\n",
        "      delta =  2./ (code_num-1)\n",
        "      ch = channel // in_features\n",
        "      t = torch.arange(-1, 1 + 0.1 * delta, step=delta)\n",
        "      t = t.unsqueeze(0).repeat(ch, 1)\n",
        "      for i in range(in_features):\n",
        "        n = (torch.rand_like(t) - 0.5) * 1.0e-2\n",
        "        shape_code[i*ch:(i+1)*ch, i*code_num:(i+1)*code_num] = t + n\n",
        "      self.pos_enc.shape_code.copy_(shape_code)\n",
        "  \n",
        "  def forward(self, input):\n",
        "    coords = self.proj(input)\n",
        "    enc = self.pos_enc(coords)\n",
        "    output = self.net(enc)[0]\n",
        "    return output, input.clone().detach().requires_grad_(True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6HY9npaslE8"
      },
      "source": [
        "And finally, differential operators that allow us to leverage autograd to compute gradients, the laplacian, etc. For this project, we only use gradient to calculat the TV regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8xDyatwslE8"
      },
      "outputs": [],
      "source": [
        "def laplace(y, x):\n",
        "    grad = gradient(y, x)\n",
        "    return divergence(grad, x)\n",
        "\n",
        "\n",
        "def divergence(y, x):\n",
        "    div = 0.\n",
        "    for i in range(y.shape[-1]):\n",
        "        div += torch.autograd.grad(y[..., i], x, torch.ones_like(y[..., i]), create_graph=True)[0][..., i:i+1]\n",
        "    return div\n",
        "\n",
        "\n",
        "def gradient(y, x, grad_outputs=None):\n",
        "    if grad_outputs is None:\n",
        "        grad_outputs = torch.ones_like(y)\n",
        "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True, allow_unused=True)[0]\n",
        "    return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taBAOOaIslE9"
      },
      "source": [
        "## Experiments Setup\n",
        "\n",
        "We will use an image from the BSDS300 dataset. First, let's set up the image loader and add noise to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSMDYywQBFCC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import skimage.io as io\n",
        "bsds300_dir = '/content/drive/My Drive/Colab Notebooks/BSDS300/images'\n",
        "output_dir = '/content/drive/My Drive/Colab Notebooks/siren_out'\n",
        "\n",
        "def get_bsds_tensor(sidelength, split='test', id='108005'):\n",
        "  dir = os.path.join(bsds300_dir, split, f'{id}.jpg')\n",
        "  img = torch.tensor(io.imread(dir) / 255).float()\n",
        "\n",
        "  # Resize image\n",
        "  crop_x_l = (img.shape[0] - sidelength) // 2\n",
        "  crop_x_r = sidelength + crop_x_l\n",
        "  crop_y_l = (img.shape[1] - sidelength) // 2\n",
        "  crop_y_r = sidelength + crop_y_l\n",
        "  img = img[crop_x_l:crop_x_r, crop_y_l:crop_y_r, :]\n",
        "\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "YOljstHIBQ_Z",
        "outputId": "c4564d4c-c7ad-45f2-bd29-3b2270cd4659"
      },
      "outputs": [],
      "source": [
        "img = get_bsds_tensor(256)\n",
        "print(img.shape)\n",
        "fig = plt.figure()\n",
        "plt.imshow(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JSmHYRW-zw-"
      },
      "source": [
        "Add Gaussian noise to this image. Show the resulting PSNR between the noisy and clean image, which will be the baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkDWFl0Yx_AU"
      },
      "outputs": [],
      "source": [
        "def get_noisy_image(img, sigma):\n",
        "    \"\"\"Adds Gaussian noise to an image.\n",
        "    \n",
        "    Args: \n",
        "        img: image, torch.tensor with values from 0 to 1\n",
        "        sigma: std of the noise\n",
        "    \"\"\"\n",
        "    noise = torch.normal(torch.zeros_like(img), sigma * torch.ones_like(img))\n",
        "    img_noisy = torch.clip(img + noise, 0, 1).float()\n",
        "    #img_noisy = img + noise\n",
        "\n",
        "    return img_noisy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "_ALf-JSvNjLB",
        "outputId": "744cd39f-6d52-4ed1-d1ca-efe6c4f10064"
      },
      "outputs": [],
      "source": [
        "img_noisy = get_noisy_image(img, 0.1).numpy()\n",
        "fig = plt.figure()\n",
        "plt.imshow(img_noisy)\n",
        "psnr_noisy_clean = compare_psnr(img.numpy(), img_noisy)\n",
        "print(f\"PSNR between clean and noisy image: {psnr_noisy_clean}\")\n",
        "lpips_noisy_clean = lpips_np(img.numpy(), img_noisy)\n",
        "print(f\"LPIPS between clean and noisy image: {lpips_noisy_clean}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkZth41OITKb",
        "outputId": "7a00b5b8-ad5b-4b53-fb88-8feb7c565f74"
      },
      "outputs": [],
      "source": [
        "io.imsave(os.path.join(output_dir, 'img_noisy.jpg'), img_noisy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRV9bvN2slE-"
      },
      "outputs": [],
      "source": [
        "class ImageFitting(Dataset):\n",
        "    def __init__(self, sidelength, split='test', id='108005'):\n",
        "        super().__init__()\n",
        "        self.clean_pixels = get_bsds_tensor(sidelength, split, id)\n",
        "        self.coords = get_mgrid(sidelength, 2)\n",
        "\n",
        "        # Add noise\n",
        "        self.pixels = get_noisy_image(self.clean_pixels, 0.1)\n",
        "\n",
        "        # Normalize as the target to train on\n",
        "        #transform = Normalize(torch.Tensor([0.5]), torch.Tensor([0.5]))\n",
        "        #self.pixels = transform(self.pixels)\n",
        "\n",
        "        # Flatten spatial dimension for training\n",
        "        self.pixels = self.pixels.reshape(-1, 3)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "    def __getitem__(self, idx):    \n",
        "        if idx > 0: raise IndexError\n",
        "            \n",
        "        return self.coords, self.pixels\n",
        "      \n",
        "    def get_clean_img(self):\n",
        "      return self.clean_pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDtWJA1ZdOw_"
      },
      "source": [
        "We define the TV prior loss that can be used as regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtujiqL_dSVf"
      },
      "outputs": [],
      "source": [
        "def image_mse_TV_prior(k1, model, model_output, coords, gt_img):\n",
        "  \"\"\"Calculate loss with TV prior regularizaton\n",
        "  Args:\n",
        "    k1 (float): Weight of TV regularization term\n",
        "    model (nn.Module): SIREN model\n",
        "    model_output (tensor): Output values at query `coords`\n",
        "    coords (tensor): Coordinates that we query the model to get `model_output`\n",
        "    gt_img (tensor): Ground-truth image values at `coords`\n",
        "  \"\"\"\n",
        "  # Query random coordinates for TV loss calculation\n",
        "  coords_rand = 2 * (torch.rand((coords.shape[0],\n",
        "                                 coords.shape[1] // 2,\n",
        "                                 coords.shape[2])).cuda() - 0.5)\n",
        "  model_out_rand, model_in_rand = model(coords_rand)\n",
        "\n",
        "  return {'img_loss': ((model_output - gt_img) ** 2).mean(),\n",
        "          'prior_loss': k1 * (torch.abs(gradient(\n",
        "            model_out_rand, model_in_rand))).mean()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr5MRSThslE_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Let's instantiate the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "pLkx5peTslE_",
        "outputId": "31fac6e6-a735-4fa7-a22b-27941056ae45"
      },
      "outputs": [],
      "source": [
        "tiger = ImageFitting(256)\n",
        "img_clean = tiger.get_clean_img().numpy()\n",
        "dataloader = DataLoader(tiger, batch_size=1, pin_memory=True, num_workers=0)\n",
        "model_input, ground_truth = next(iter(dataloader))\n",
        "model_input, ground_truth = model_input.cuda(), ground_truth.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4-QrtawslE_"
      },
      "source": [
        "This is the main training loop for image fitting. With different `loss` parameter, we can either fit the noisy image directly, or add regularizaiton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4pjtnPgWvfK"
      },
      "outputs": [],
      "source": [
        "def train(img_siren, loss_type='fidelity', k1=1e-4, total_steps=1000, steps_til_summary=20, early_stop=600, spe=False, lr=1e-4):\n",
        "  assert loss_type in ('fidelity', 'tv')\n",
        "\n",
        "  optim = torch.optim.Adam(lr=lr, params=img_siren.parameters())\n",
        "  model_input, ground_truth = next(iter(dataloader))\n",
        "\n",
        "  # For visualization\n",
        "  img_in = ground_truth.view(256, 256, -1).numpy()\n",
        "  bbox_caption = {'facecolor': 'white', 'pad': 8}\n",
        "\n",
        "  model_input, ground_truth = model_input.cuda(), ground_truth.cuda()\n",
        "\n",
        "  # Save for record\n",
        "  psnrs_clean = []\n",
        "  psnrs_noisy = []\n",
        "  lpipss_clean = []\n",
        "  lpipss_noisy = []\n",
        "  tvs = []\n",
        "\n",
        "  best_psnr_clean = -float('inf')\n",
        "  best_step = 0\n",
        "\n",
        "  model_tag = 'spe_' if spe else ''\n",
        "\n",
        "  for step in range(total_steps + 1):\n",
        "      model_output, coords = img_siren(model_input)\n",
        "      if loss_type == 'fidelity':\n",
        "        loss = ((model_output - ground_truth)**2).mean()\n",
        "      else:\n",
        "        loss_dict = image_mse_TV_prior(k1, img_siren, model_output, coords, ground_truth)\n",
        "        loss = loss_dict['img_loss'] + loss_dict['prior_loss']\n",
        "        tvs.append(loss_dict['prior_loss'].detach().cpu().numpy() / k1)\n",
        "      \n",
        "      # Compute PSNR metric\n",
        "      img_out = model_output.cpu().view(256, 256, -1).detach().numpy()\n",
        "      psnr_noisy = compare_psnr(img_in, img_out)\n",
        "      psnrs_noisy.append(psnr_noisy)\n",
        "      psnr_clean = compare_psnr(img_clean, img_out)\n",
        "      psnrs_clean.append(psnr_clean)\n",
        "\n",
        "      # TODO: have a better stopping criterion because we cannot assume access to\n",
        "      # underlying clean image\n",
        "      if best_psnr_clean < psnr_clean:\n",
        "        best_psnr_clean = psnr_clean\n",
        "        best_step = step\n",
        "        torch.save(img_siren.state_dict(), \n",
        "                   os.path.join(output_dir, f\"{loss_type}_{model_tag}best.pt\"))\n",
        "       \n",
        "       # Save early stop model\n",
        "      if step == early_stop or step == total_steps:\n",
        "        torch.save(img_siren.state_dict(), \n",
        "                   os.path.join(output_dir, f\"{loss_type}_{model_tag}step{early_stop}.pt\"))\n",
        "\n",
        "      if not step % steps_til_summary:\n",
        "          print(\"Step %d, Total loss %0.6f\" % (step, loss))\n",
        "          if spe:\n",
        "            img_grad = torch.zeros_like(model_output)\n",
        "          else:\n",
        "            img_grad = gradient(model_output, coords)\n",
        "          # img_laplacian = laplace(model_output, coords)\n",
        "          \n",
        "          # TODO: have better stopping criterion?\n",
        "          # Save best model?\n",
        "          # Compute LPIPS metric\n",
        "          with torch.no_grad():\n",
        "            lpips_noisy = lpips_np(img_in, img_out)\n",
        "            lpipss_noisy.append(lpips_noisy)\n",
        "            lpips_clean = lpips_np(img_clean, img_out)\n",
        "            lpipss_clean.append(lpips_clean)\n",
        "          \n",
        "          # Visualization\n",
        "          fig, axes = plt.subplots(1, 4, figsize=(24,6))\n",
        "          axes[0].imshow(img_out)\n",
        "          axes[0].text(12, 20, f'Model output. PSNR_noisy={round(psnr_noisy, 2)}, PSNR_clean={round(psnr_clean, 2)}', bbox=bbox_caption)\n",
        "          axes[0].text(12, 50, f'LPIPS_noisy={round(lpips_noisy, 2)}, LPIPS_clean={round(lpips_clean, 2)}', bbox=bbox_caption)\n",
        "          axes[1].imshow(img_grad.norm(dim=-1).cpu().view(256, 256).detach().numpy())\n",
        "          axes[1].text(12, 20, 'Model output gradient', bbox=bbox_caption)\n",
        "          #axes[2].imshow(img_laplacian.cpu().view(256, 256).detach().numpy())\n",
        "          axes[2].imshow(img_in)\n",
        "          axes[2].text(12, 20, f'Model target (noisy). PSNR_clean={round(psnr_noisy_clean, 2)}', bbox=bbox_caption)\n",
        "          axes[2].text(12, 50, f'LPIPS_clean={round(lpips_noisy_clean, 2)}', bbox=bbox_caption)\n",
        "          axes[3].imshow(img_clean)\n",
        "          axes[3].text(12, 20, 'Clean image', bbox=bbox_caption)\n",
        "          plt.savefig(os.path.join(output_dir, loss_type, f'{loss_type}_{model_tag}step{step}.png'), dpi=300)\n",
        "\n",
        "          # Show image at lower frequency\n",
        "          if not step % (steps_til_summary * 5):\n",
        "            plt.show()\n",
        "          \n",
        "          plt.close()\n",
        "\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "\n",
        "  return {'psnrs_clean': psnrs_clean, 'psnrs_noisy': psnrs_noisy, \n",
        "          'lpipss_clean': lpipss_clean, 'lpipss_noisy': lpipss_noisy, \n",
        "          'tvs': tvs, 'best_step': best_step}\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6FG9c-XBjs0"
      },
      "source": [
        "Finally, we define all the models we need to run the experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rnvz5PsMBjKV"
      },
      "outputs": [],
      "source": [
        "img_siren = Siren(in_features=2, out_features=3, hidden_features=256, \n",
        "                     hidden_layers=3, outermost_linear=True)\n",
        "img_siren.cuda()\n",
        "\n",
        "img_siren_tv = Siren(in_features=2, out_features=3, hidden_features=256, \n",
        "                     hidden_layers=3, outermost_linear=True)\n",
        "img_siren_tv.cuda()\n",
        "\n",
        "img_siren_spe = PosEncSiren(in_features=2, out_features=3, hidden_features=256, \n",
        "                           hidden_layers=3)\n",
        "img_siren_spe.cuda();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB_mLYtraRUI"
      },
      "source": [
        "## Model 1: Fitting the noisy image directly\n",
        "\n",
        "First, let's simply fit that noisy image!\n",
        "\n",
        "We seek to parameterize a corrupted RGb image $\\hat{I}(x)$ with pixel coordinates $x$ with a SIREN $\\Phi(x)$.\n",
        "\n",
        "That is we seek the function $\\Phi$ such that:\n",
        "$\\mathcal{L}_{\\text{fidelity}} = \\int \\Vert \\Phi(\\mathbf{x}) - \\hat{I}(\\mathbf{x})\\Vert_2 d\\mathbf{x}$\n",
        " is minimized, in which $\\Omega$ is the domain of the image. \n",
        "\n",
        "During training, we visualize the fitted image output (`Model output`), the gradient of the fitted image (`Model output gradient`), the target noisy image that the model is fitting to (`Model target (noisy)`), and the underlying clean image that the model never sees (`Clean image`).\n",
        "\n",
        "We also show the `PSNR_noisy`, the PSNR between the current image to the noisy target image, and `PSNR_clean`, the PSNR between the current image to the underlying clean image. We do the same for LPIPS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x4q_7TcCXmWD",
        "outputId": "ee163bff-9953-4c00-f19b-e8c091f32858"
      },
      "outputs": [],
      "source": [
        "total_steps=1200\n",
        "steps_til_summary=20\n",
        "train_out = train(img_siren, loss_type='fidelity', \n",
        "                  total_steps=total_steps, \n",
        "                  steps_til_summary=steps_til_summary)\n",
        "psnrs_clean = train_out['psnrs_clean']\n",
        "psnrs_noisy = train_out['psnrs_noisy']\n",
        "lpipss_clean = train_out['lpipss_clean']\n",
        "lpipss_noisy = train_out['lpipss_noisy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jloUAnIEbNmP"
      },
      "source": [
        "Plot the trajectory of PSNR to clean image and PSNR to noisy image. We see that even though the model never sees the clean image and only fits to the noisy image, it is able to avoid fitting the noise have a high `PSNR_clean` in the first few hundred iterations, before eventually starting to fit the noise and decreasing the PSNR (starting at around iteration 500)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "VkN6rPxXUb7Z",
        "outputId": "14c1d3ba-588c-4b57-9c4a-2ac5730698dd"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "plt.plot(psnrs_clean, label=\"PSNR_clean (fidelity)\")\n",
        "plt.plot(psnrs_noisy, label=\"PSNR_noisy (fidelity)\")\n",
        "plt.plot(np.ones(len(psnrs_noisy)) * psnr_noisy_clean, label=\"PSNR of noisy to clean\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"PSNR for training with data fidelity\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'fidelity_PSNR.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "c8Azf8KgKB4K",
        "outputId": "63d0dd6f-6026-44b8-d82d-cb3a298abd6e"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), lpipss_clean, label=\"LPIPS_clean (fidelity)\")\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), lpipss_noisy, label=\"LPIPS_noisy (fidelity)\")\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), np.ones(len(lpipss_noisy)) * lpips_noisy_clean, label=\"LPIPS (alex) of noisy to clean\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"LPIPS\")\n",
        "plt.title(\"LPIPS metric for training with data fidelity\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'fidelity_LPIPS.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmH5552PcpSi"
      },
      "source": [
        "## Model 2: Fitting the noisy image with TV prior\n",
        "\n",
        "Next, we add TV regularization for fitting the image. The new loss function becomes\n",
        "\n",
        "$\\mathcal{L}_{\\text{tv}} = \\int \\Vert \\Phi(\\mathbf{x}) - \\hat{I}(\\mathbf{x})\\Vert_2 + \\kappa \\Vert \\nabla_{\\mathbf{x}}\\Phi(\\mathbf{x})\\Vert_1  d\\mathbf{x}$,\n",
        "where $\\kappa$ is a hyperparameter for the TV regularization weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GDRMEdbyhFd8",
        "outputId": "33d14dd8-4806-4dbe-8ea1-eb2aeb62399f"
      },
      "outputs": [],
      "source": [
        "total_steps=2400\n",
        "steps_til_summary=40\n",
        "train_out_tv = train(img_siren_tv, loss_type='tv', k1=5e-4,\n",
        "                  total_steps=total_steps, \n",
        "                  steps_til_summary=steps_til_summary,\n",
        "                  early_stop=2000, lr=1e-3)\n",
        "psnrs_clean_tv = train_out_tv['psnrs_clean']\n",
        "psnrs_noisy_tv = train_out_tv['psnrs_noisy']\n",
        "lpipss_clean_tv = train_out_tv['lpipss_clean']\n",
        "lpipss_noisy_tv = train_out_tv['lpipss_noisy']\n",
        "tvs = train_out_tv['tvs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "xXNIYS2uij7t",
        "outputId": "761803d8-5fd4-4486-8d07-612cd3dddd88"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "plt.plot(psnrs_clean_tv, label=\"PSNR_clean (TV)\")\n",
        "plt.plot(psnrs_noisy_tv, label=\"PSNR_noisy (TV)\")\n",
        "\n",
        "plt.plot(np.ones(len(psnrs_noisy_tv)) * psnr_noisy_clean, label=\"PSNR of noisy to clean\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"PSNR for traning with TV regularizer\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'tv_PSNR.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "_eAksc884LPS",
        "outputId": "45d30823-9fe6-4e12-badb-8e78dd40caa2"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), lpipss_clean_tv[:-1], label=\"LPIPS_clean (TV)\")\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), lpipss_noisy_tv[:-1], label=\"LPIPS_noisy (TV)\")\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), np.ones(len(lpipss_noisy_tv[:-1])) * lpips_noisy_clean, label=\"LPIPS (alex) of noisy to clean\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"LPIPS\")\n",
        "plt.title(\"LPIPS for training with TV regularizer\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'tv_LPIPS.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "7EjHleyljKJl",
        "outputId": "f64881e1-80d5-4e8d-9274-d91ebc72e73a"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "plt.plot(tvs)\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.title(\"Total variation\")\n",
        "plt.savefig(os.path.join(output_dir, 'tv.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfsUuU64-rJr"
      },
      "source": [
        "## Model 3: Fitting Noisy Image with Spline Coordinate Encoding\n",
        "Finally, we encode the input coordinates with spline coordinate encoding before passing it to SIREN. The new loss function is $\\mathcal{L}_{\\text{spe}} = \\int \\Vert \\Phi(S(\\mathbf{x})) - \\hat{I}(\\mathbf{x})\\Vert_2  d\\mathbf{x}$, where $S$ is the spline coordinate projection function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "_oaBkAFu-xHE",
        "outputId": "532b0d54-3a2f-4e76-b193-6d8af8ecc19b"
      },
      "outputs": [],
      "source": [
        "total_steps=2400\n",
        "steps_til_summary=40\n",
        "train_out_spe = train(img_siren_spe, loss_type='fidelity', k1=5e-5,\n",
        "                  total_steps=total_steps, \n",
        "                  steps_til_summary=steps_til_summary, spe=True,\n",
        "                  early_stop=2000, lr=1e-3)\n",
        "psnrs_clean_spe = train_out_spe['psnrs_clean']\n",
        "psnrs_noisy_spe = train_out_spe['psnrs_noisy']\n",
        "lpipss_clean_spe = train_out_spe['lpipss_clean']\n",
        "lpipss_noisy_spe = train_out_spe['lpipss_noisy']\n",
        "tvs_spe = train_out_spe['tvs']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "E1_IAdb_zC4t",
        "outputId": "7c3732da-ad93-44af-f16d-597a6d36a7bb"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "plt.plot(psnrs_clean_spe, label=\"PSNR_clean (SPE)\")\n",
        "plt.plot(psnrs_noisy_spe, label=\"PSNR_noisy (SPE)\")\n",
        "\n",
        "plt.plot(np.ones(len(psnrs_noisy_spe)) * psnr_noisy_clean, label=\"PSNR of noisy to clean\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"PSNR for traning with SPE\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'spe_PSNR.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "FeMQTAnezoBr",
        "outputId": "5733cd00-370e-419b-d3b7-110419d0b74a"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), lpipss_clean_spe[:-1], label=\"LPIPS_clean (SPE)\")\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), lpipss_noisy_spe[:-1], label=\"LPIPS_noisy (SPE)\")\n",
        "plt.plot(np.arange(0, total_steps, steps_til_summary), np.ones(len(lpipss_noisy_spe[:-1])) * lpips_noisy_clean, label=\"LPIPS (alex) of noisy to clean\")\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"LPIPS\")\n",
        "plt.title(\"LPIPS for training with SPE\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'spe_LPIPS.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHOZegK7009y"
      },
      "source": [
        "## Summary of Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "HnRBEnmv2M50",
        "outputId": "b5a30d03-1dcd-40f8-daf9-946930349af6"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "\n",
        "plt.plot(np.arange(0, 1200), psnrs_clean, label=\"PSNR_clean (fidelity)\")\n",
        "plt.plot(np.arange(0, 2400), psnrs_clean_tv[:-1], label=\"PSNR_clean (TV)\")\n",
        "psnrs_clean_spe_smoothed = np.convolve(psnrs_clean_spe[:-1], np.ones(20), 'valid') / 20\n",
        "plt.plot(np.arange(0, 2400-19), psnrs_clean_spe_smoothed, label=\"PSNR_clean (SPE, smoothed)\")\n",
        "\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"PSNR comparison\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'comparison_PSNR.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "urH6GfXW05Hl",
        "outputId": "746f0e84-2193-4251-e9dc-f6c99dfeaa3d"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "\n",
        "plt.plot(np.arange(0, 1200, 20), lpipss_clean, label=\"LPIPS_clean (fidelity)\")\n",
        "plt.plot(np.arange(0, 2400, 40), lpipss_clean_tv[:-1], label=\"LPIPS_clean (TV)\")\n",
        "plt.plot(np.arange(0, 2400, 40), lpipss_clean_spe[:-1], label=\"LPIPS_clean (SPE)\")\n",
        "\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"LPIPS\")\n",
        "plt.title(\"LPIPS comparison\")\n",
        "plt.legend()\n",
        "plt.savefig(os.path.join(output_dir, 'comparison_LPIPS.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gISHbzQJEkNv"
      },
      "source": [
        "### Load Trained Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8bwJgKgEjSI"
      },
      "outputs": [],
      "source": [
        "fidelity_stop = 600\n",
        "tv_stop = 2000\n",
        "spe_stop = 2000\n",
        "\n",
        "img_siren.load_state_dict(torch.load(os.path.join(output_dir, f'fidelity_step{fidelity_stop}.pt')))\n",
        "img_siren_tv.load_state_dict(torch.load(os.path.join(output_dir, f'tv_step{tv_stop}.pt')))\n",
        "img_siren_spe.load_state_dict(torch.load(os.path.join(output_dir, f'fidelity_spe_step{spe_stop}.pt')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm9AYExk3Vlw"
      },
      "source": [
        "# Qualitative Comparison of SIREN Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ThvhGPg3UeE"
      },
      "outputs": [],
      "source": [
        "fidelity_stop = 600\n",
        "tv_stop = 2000\n",
        "spe_stop = 2000\n",
        "\n",
        "with torch.no_grad():\n",
        "  # Get fidelity results\n",
        "  img_siren.eval()\n",
        "  model_output, _ = img_siren(model_input)\n",
        "  img_out_fidelity = model_output.cpu().view(256, 256, -1).detach().numpy()\n",
        "  fidelity_best_psnr = compare_psnr(img_clean, img_out_fidelity)\n",
        "  fidelity_best_lpips = lpips_np(img_clean, img_out_fidelity)\n",
        "  \n",
        "  # Get tv results\n",
        "  img_siren_tv.eval()\n",
        "  model_output, _ = img_siren_tv(model_input)\n",
        "  img_out_tv = model_output.cpu().view(256, 256, -1).detach().numpy()\n",
        "  tv_best_psnr = compare_psnr(img_clean, img_out_tv)\n",
        "  tv_best_lpips = lpips_np(img_clean, img_out_tv)\n",
        "\n",
        "  # Get spe results\n",
        "  img_siren_spe.eval()\n",
        "  model_output, _ = img_siren_spe(model_input)\n",
        "  img_out_spe = model_output.cpu().view(256, 256, -1).detach().numpy()\n",
        "  spe_best_psnr = compare_psnr(img_clean, img_out_spe)\n",
        "  spe_best_lpips = lpips_np(img_clean, img_out_spe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8QtA4OT7ssC",
        "outputId": "a820d66d-d24b-4842-f271-99288e11a2d1"
      },
      "outputs": [],
      "source": [
        "print(f\"fidelity_best_psnr: {fidelity_best_psnr}\")\n",
        "print(f\"tv_best_psnr: {tv_best_psnr}\")\n",
        "print(f\"spe_best_psnr: {spe_best_psnr}\")\n",
        "print(f\"fidelity_best_lpips: {fidelity_best_lpips}\")\n",
        "print(f\"tv_best_lpips: {tv_best_lpips}\")\n",
        "print(f\"spe_best_lpips: {spe_best_lpips}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "cU4Zch6u7daI",
        "outputId": "e25ca408-3d2f-4619-d150-ed679dee8ade"
      },
      "outputs": [],
      "source": [
        "bbox_caption = {'facecolor': 'white', 'pad': 8}\n",
        "fig, axes = plt.subplots(1, 5, figsize=(30, 6))\n",
        "\n",
        "axes[0].imshow(img_noisy)\n",
        "axes[0].text(12, 20, f'Noisy image. PSNR_clean={round(psnr_noisy_clean, 3)}', bbox=bbox_caption)\n",
        "axes[0].text(12, 50, f'LPIPS_clean={round(lpips_noisy_clean, 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[1].imshow(img_out_fidelity)\n",
        "axes[1].text(12, 20, f'Fidelity model. PSNR_clean={round(fidelity_best_psnr, 3)}', bbox=bbox_caption)\n",
        "axes[1].text(12, 50, f'LPIPS_clean={round(fidelity_best_lpips, 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[2].imshow(img_out_tv)\n",
        "axes[2].text(12, 20, f'TV model. PSNR_clean={round(tv_best_psnr, 3)}', bbox=bbox_caption)\n",
        "axes[2].text(12, 50, f'LPIPS_clean={round(tv_best_lpips, 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[3].imshow(img_out_spe)\n",
        "axes[3].text(12, 20, f'SPE model. PSNR_clean={round(spe_best_psnr, 3)}', bbox=bbox_caption)\n",
        "axes[3].text(12, 50, f'LPIPS_clean={round(spe_best_lpips, 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[4].imshow(img_clean)\n",
        "axes[4].text(12, 20, 'Clean image', bbox=bbox_caption)\n",
        "\n",
        "plt.savefig(os.path.join(output_dir, 'denoised_comparison.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-fIbfPKcgWD"
      },
      "source": [
        "# Model 4: Continuous Bilateral Filtering on SIREN representation\n",
        "\n",
        "Here we implement a continuous version of bilateral filtering on the learned SIREN model. See paper for details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pP6sEzhidzzE"
      },
      "outputs": [],
      "source": [
        "def siren_blur(model, n_average=25, scale=0.05, sigma_loc=0.05, sigma_int=0.05):\n",
        "  model_input, _ = next(iter(dataloader))\n",
        "  model_input = model_input.cuda()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    # Calculate clean input PSNR\n",
        "    model_output, _ = model(model_input)\n",
        "    img_out_gt = model_output.cpu().view(256, 256, -1).detach().numpy()\n",
        "\n",
        "    # Calculate multiple responses\n",
        "    model_input = model_input.repeat(n_average, 1, 1)\n",
        "    # noise = torch.normal(torch.zeros_like(model_input), torch.ones_like(model_input) * std)\n",
        "    noise = (torch.rand(model_input.shape, device='cuda') - 0.5) * scale\n",
        "    model_output, coords = model(model_input + noise)\n",
        "    img_out_mult = model_output.cpu().view(n_average, 256, 256, -1).detach().cpu().numpy()\n",
        "\n",
        "    # Calculate weight of each response\n",
        "    location_weight = np.exp(-np.linalg.norm(noise.cpu().numpy(), axis=-1)**2 / (2 * sigma_loc**2))\n",
        "    location_weight = location_weight.reshape(n_average, 256, 256, 1)\n",
        "    intensity_weight = np.exp(-(img_out_mult - img_out_gt)**2 / (2 * sigma_int**2))\n",
        "    weight = location_weight * intensity_weight\n",
        "    weight /= weight.mean(0)\n",
        "    \n",
        "    final_img_out = (weight * img_out_mult).mean(0)\n",
        "\n",
        "  return final_img_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vxr0EnnH-r9E"
      },
      "outputs": [],
      "source": [
        "img_out_fidelity_blur = siren_blur(img_siren, scale=0.1, sigma_loc=0.06, sigma_int=0.06)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "LbuxpzDTxEgA",
        "outputId": "8fff7dd3-669d-4949-9d29-31c0df949f9a"
      },
      "outputs": [],
      "source": [
        "bbox_caption = {'facecolor': 'white', 'pad': 8}\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(img_out_fidelity)\n",
        "axes[0].text(12, 20, f'Fidelity model. PSNR_clean={round(compare_psnr(img_clean, img_out_fidelity), 3)}', bbox=bbox_caption)\n",
        "axes[0].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_out_fidelity), 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[1].imshow(img_out_fidelity_blur)\n",
        "axes[1].text(12, 20, f'Fidelity model blurred. PSNR_clean={round(compare_psnr(img_clean, img_out_fidelity_blur), 3)}', bbox=bbox_caption)\n",
        "axes[1].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_out_fidelity_blur), 3)}', bbox=bbox_caption)\n",
        "\n",
        "plt.savefig(os.path.join(output_dir, 'fidelity_blurred.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gspB-x3zD4sS"
      },
      "outputs": [],
      "source": [
        "img_out_tv_blur = siren_blur(img_siren_tv, scale=0.1, sigma_loc=0.06, sigma_int=0.06)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "JiPBN5k5D89f",
        "outputId": "6adf1eab-1a05-4876-b9fe-e1590d122d59"
      },
      "outputs": [],
      "source": [
        "bbox_caption = {'facecolor': 'white', 'pad': 8}\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(img_out_tv)\n",
        "axes[0].text(12, 20, f'TV model. PSNR_clean={round(compare_psnr(img_clean, img_out_tv), 3)}', bbox=bbox_caption)\n",
        "axes[0].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_out_tv), 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[1].imshow(img_out_tv_blur)\n",
        "axes[1].text(12, 20, f'TV model blurred. PSNR_clean={round(compare_psnr(img_clean, img_out_tv_blur), 3)}', bbox=bbox_caption)\n",
        "axes[1].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_out_tv_blur), 3)}', bbox=bbox_caption)\n",
        "\n",
        "plt.savefig(os.path.join(output_dir, 'tv_blurred.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIbB-vdm4msL"
      },
      "source": [
        "Note: ideally I would run SPE as well but the free version of Colab cannot really handle it at a reasonale speed using CUDA without running out of memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PYCsrtr5Slj"
      },
      "source": [
        "# Baseline Comparisons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdp1dLtTD3oP"
      },
      "source": [
        "## Baseline 1: BM3D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umN_vwrJFiBM",
        "outputId": "5fb5fde1-2035-4fcc-a079-56b77128bc58"
      },
      "outputs": [],
      "source": [
        "!pip install bm3d\n",
        "import bm3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u81j_SXxFsvu"
      },
      "outputs": [],
      "source": [
        "img_out_bm3d = bm3d.bm3d(img_noisy, sigma_psd=25/255).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "LkgYcdRcGDfP",
        "outputId": "78c22a0c-7a49-4d35-fb7d-88594679f74b"
      },
      "outputs": [],
      "source": [
        "bbox_caption = {'facecolor': 'white', 'pad': 8}\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(img_noisy)\n",
        "axes[0].text(12, 20, f'Noisy image. PSNR_clean={round(compare_psnr(img_clean, img_noisy), 3)}', bbox=bbox_caption)\n",
        "axes[0].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_noisy), 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[1].imshow(img_out_bm3d)\n",
        "axes[1].text(12, 20, f'BM3D model. PSNR_clean={round(compare_psnr(img_clean, img_out_bm3d), 3)}', bbox=bbox_caption)\n",
        "axes[1].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_out_bm3d), 3)}', bbox=bbox_caption)\n",
        "\n",
        "plt.savefig(os.path.join(output_dir, 'denoised_bm3d.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTg7PO4wG-_8"
      },
      "source": [
        "## Baseline 2: Deep Image Prior\n",
        "\n",
        "Here we directly load the results of trained Deep Image Prior. See their project page for a Colab notebook on how to train it yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "WMerdGbeG-aj",
        "outputId": "da22dd17-5971-42f8-8af1-43115b6e6428"
      },
      "outputs": [],
      "source": [
        "img_out_dip = (io.imread(os.path.join(output_dir, 'dip_out.jpg')) / 255).astype(np.float32)\n",
        "\n",
        "bbox_caption = {'facecolor': 'white', 'pad': 8}\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(img_noisy)\n",
        "axes[0].text(12, 20, f'Noisy image. PSNR_clean={round(compare_psnr(img_clean, img_noisy), 3)}', bbox=bbox_caption)\n",
        "axes[0].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_noisy), 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[1].imshow(img_out_dip)\n",
        "axes[1].text(12, 20, f'Deep Image Prior model. PSNR_clean={round(compare_psnr(img_clean, img_out_dip), 3)}', bbox=bbox_caption)\n",
        "axes[1].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_out_dip), 3)}', bbox=bbox_caption)\n",
        "\n",
        "plt.savefig(os.path.join(output_dir, 'denoised_dip.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46BwNUloTSfr"
      },
      "source": [
        "## Baseline 3: Denoising Convolutional Neural Network (DnCNN)\n",
        "Similarly, I load a DnCNN that is already trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYEuO5SGTVXz"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "def sequential(*args):\n",
        "    \"\"\"Advanced nn.Sequential.\n",
        "\n",
        "    Args:\n",
        "        nn.Sequential, nn.Module\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential\n",
        "    \"\"\"\n",
        "    if len(args) == 1:\n",
        "        if isinstance(args[0], OrderedDict):\n",
        "            raise NotImplementedError('sequential does not support OrderedDict input.')\n",
        "        return args[0]  # No sequential is needed.\n",
        "    modules = []\n",
        "    for module in args:\n",
        "        if isinstance(module, nn.Sequential):\n",
        "            for submodule in module.children():\n",
        "                modules.append(submodule)\n",
        "        elif isinstance(module, nn.Module):\n",
        "            modules.append(module)\n",
        "    return nn.Sequential(*modules)\n",
        "\n",
        "# --------------------------------------------\n",
        "# return nn.Sequantial of (Conv + BN + ReLU)\n",
        "# --------------------------------------------\n",
        "def conv(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=True, mode='CBR', negative_slope=0.2):\n",
        "    L = []\n",
        "    for t in mode:\n",
        "        if t == 'C':\n",
        "            L.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias))\n",
        "        elif t == 'T':\n",
        "            L.append(nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias))\n",
        "        elif t == 'B':\n",
        "            L.append(nn.BatchNorm2d(out_channels, momentum=0.9, eps=1e-04, affine=True))\n",
        "        elif t == 'I':\n",
        "            L.append(nn.InstanceNorm2d(out_channels, affine=True))\n",
        "        elif t == 'R':\n",
        "            L.append(nn.ReLU(inplace=True))\n",
        "        elif t == 'r':\n",
        "            L.append(nn.ReLU(inplace=False))\n",
        "        elif t == 'L':\n",
        "            L.append(nn.LeakyReLU(negative_slope=negative_slope, inplace=True))\n",
        "        elif t == 'l':\n",
        "            L.append(nn.LeakyReLU(negative_slope=negative_slope, inplace=False))\n",
        "        elif t == '2':\n",
        "            L.append(nn.PixelShuffle(upscale_factor=2))\n",
        "        elif t == '3':\n",
        "            L.append(nn.PixelShuffle(upscale_factor=3))\n",
        "        elif t == '4':\n",
        "            L.append(nn.PixelShuffle(upscale_factor=4))\n",
        "        elif t == 'U':\n",
        "            L.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
        "        elif t == 'u':\n",
        "            L.append(nn.Upsample(scale_factor=3, mode='nearest'))\n",
        "        elif t == 'v':\n",
        "            L.append(nn.Upsample(scale_factor=4, mode='nearest'))\n",
        "        elif t == 'M':\n",
        "            L.append(nn.MaxPool2d(kernel_size=kernel_size, stride=stride, padding=0))\n",
        "        elif t == 'A':\n",
        "            L.append(nn.AvgPool2d(kernel_size=kernel_size, stride=stride, padding=0))\n",
        "        else:\n",
        "            raise NotImplementedError('Undefined type: '.format(t))\n",
        "    return sequential(*L)\n",
        "\n",
        "\n",
        "class DnCNN(nn.Module):\n",
        "    def __init__(self, in_nc=1, out_nc=1, nc=64, nb=17, act_mode='BR'):\n",
        "        \"\"\"\n",
        "        # ------------------------------------\n",
        "        in_nc: channel number of input\n",
        "        out_nc: channel number of output\n",
        "        nc: channel number\n",
        "        nb: total number of conv layers\n",
        "        act_mode: batch norm + activation function; 'BR' means BN+ReLU.\n",
        "        # ------------------------------------\n",
        "        Batch normalization and residual learning are\n",
        "        beneficial to Gaussian denoising (especially\n",
        "        for a single noise level).\n",
        "        The residual of a noisy image corrupted by additive white\n",
        "        Gaussian noise (AWGN) follows a constant\n",
        "        Gaussian distribution which stablizes batch\n",
        "        normalization during training.\n",
        "        # ------------------------------------\n",
        "        \"\"\"\n",
        "        super(DnCNN, self).__init__()\n",
        "        assert 'R' in act_mode or 'L' in act_mode, 'Examples of activation function: R, L, BR, BL, IR, IL'\n",
        "        bias = True\n",
        "\n",
        "        m_head = conv(in_nc, nc, mode='C'+act_mode[-1], bias=bias)\n",
        "        m_body = [conv(nc, nc, mode='C'+act_mode, bias=bias) for _ in range(nb-2)]\n",
        "        m_tail = conv(nc, out_nc, mode='C', bias=bias)\n",
        "\n",
        "        self.model = sequential(m_head, *m_body, m_tail)\n",
        "\n",
        "    def forward(self, x):\n",
        "        n = self.model(x)\n",
        "        return x-n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ5-M_t7T6gS"
      },
      "outputs": [],
      "source": [
        "dncnn = DnCNN(in_nc=1, out_nc=1, nc=64, nb=17, act_mode='R')\n",
        "dncnn.load_state_dict(torch.load(os.path.join(output_dir, 'dncnn_25.pth')), strict=True)\n",
        "dncnn.eval()\n",
        "with torch.no_grad():\n",
        "  dncnn_out_channels = []\n",
        "  for i in range(3):\n",
        "    out = dncnn(torch.tensor(img_noisy[:, :, i]).unsqueeze(0).unsqueeze(0)).squeeze(0).squeeze(0).cpu().numpy()\n",
        "    dncnn_out_channels.append(out)\n",
        "\n",
        "img_out_dncnn = np.stack(dncnn_out_channels, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "dVE-a4Z6WBwU",
        "outputId": "c969ffbb-1537-4103-ce27-3725c9c0ab9a"
      },
      "outputs": [],
      "source": [
        "bbox_caption = {'facecolor': 'white', 'pad': 8}\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "axes[0].imshow(img_noisy)\n",
        "axes[0].text(12, 20, f'Noisy image. PSNR_clean={round(compare_psnr(img_clean, img_noisy), 3)}', bbox=bbox_caption)\n",
        "axes[0].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_noisy), 3)}', bbox=bbox_caption)\n",
        "\n",
        "axes[1].imshow(img_out_dncnn)\n",
        "axes[1].text(12, 20, f'DnCNN model. PSNR_clean={round(compare_psnr(img_clean, img_out_dncnn), 3)}', bbox=bbox_caption)\n",
        "axes[1].text(12, 50, f'LPIPS_clean={round(lpips_np(img_clean, img_out_dncnn), 3)}', bbox=bbox_caption)\n",
        "\n",
        "plt.savefig(os.path.join(output_dir, 'denoised_dnc.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgJL91W201hn"
      },
      "source": [
        "# Quantitative Result Bar Plot\n",
        "Note: I had to hard code the values because I messed saving them when first training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nd7xU4_71fJx"
      },
      "outputs": [],
      "source": [
        "psnrs_final = {\n",
        "    'fidelity': 26.303,\n",
        "    'tv': 21.243,\n",
        "    'spe': 23.196,\n",
        "    'fidelity_blur': 27.006,\n",
        "    'dip': 28.002,\n",
        "    'bm3d': 27.488,\n",
        "    'dncnn': 27.964\n",
        "}\n",
        "\n",
        "lpipss_final = {\n",
        "    'fidelity': 0.146,\n",
        "    'tv': 0.239,\n",
        "    'spe': 0.241,\n",
        "    'fidelity_blur': 0.1022,\n",
        "    'dip': 0.103,\n",
        "    'bm3d': 0.176,\n",
        "    'dncnn': 0.126\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "qSRTB_wM3HsN",
        "outputId": "896e01ba-05e5-4780-e6dc-6864fb1dca92"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "offset = 20.122\n",
        "for i, key in enumerate(psnrs_final.keys()):\n",
        "  plt.bar(key, psnrs_final[key] - offset, bottom=offset)\n",
        "plt.xlabel(\"SIREN variants and baseline methods\")\n",
        "plt.ylabel(\"PSNR\")\n",
        "plt.title(\"Increase in PSNR after denoising\")\n",
        "plt.savefig(os.path.join(output_dir, 'psnr_bar.png'), dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "1Yc8jPnO5PAO",
        "outputId": "02f07061-a363-47b0-c928-315ed61654c3"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(1, dpi=150)\n",
        "offset = 0.263\n",
        "for i, key in enumerate(lpipss_final.keys()):\n",
        "  plt.bar(key, lpipss_final[key] - offset, bottom=offset)\n",
        "plt.xlabel(\"SIREN variants and baseline methods\")\n",
        "plt.ylabel(\"LPIPS\")\n",
        "plt.title(\"Decrease in LPIPS after denoising\")\n",
        "plt.savefig(os.path.join(output_dir, 'lpips_bar.png'), dpi=300)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NfsUuU64-rJr",
        "XHOZegK7009y",
        "gISHbzQJEkNv",
        "Jm9AYExk3Vlw"
      ],
      "name": "siren_denoise.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1159f1a1153f4142bffe4cfff80f1c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1790f43845a14dfa9512b99f37e312f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e8c663df504e72983e019ef52f3d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8479317fd1b47f3b8fd183f29d7139f",
            "max": 244408911,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_764f602d8da542dbb1d25a8df161651c",
            "value": 244408911
          }
        },
        "46a77907f1f24cdf99af2be45ccdb0f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4de6eefdf4f344349560ae1073950e66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "608aebb89a6740248df9eedf53408ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a77907f1f24cdf99af2be45ccdb0f3",
            "placeholder": "",
            "style": "IPY_MODEL_fbacc54f0ca2431195492e9915131493",
            "value": " 233M/233M [00:03&lt;00:00, 83.4MB/s]"
          }
        },
        "764f602d8da542dbb1d25a8df161651c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9eb4937ef05f433a94f46bfb059ed24e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d44e810563bf4d578ebf7ed1c61b6756",
              "IPY_MODEL_39e8c663df504e72983e019ef52f3d48",
              "IPY_MODEL_608aebb89a6740248df9eedf53408ec2"
            ],
            "layout": "IPY_MODEL_1790f43845a14dfa9512b99f37e312f6"
          }
        },
        "d44e810563bf4d578ebf7ed1c61b6756": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1159f1a1153f4142bffe4cfff80f1c2f",
            "placeholder": "",
            "style": "IPY_MODEL_4de6eefdf4f344349560ae1073950e66",
            "value": "100%"
          }
        },
        "e8479317fd1b47f3b8fd183f29d7139f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbacc54f0ca2431195492e9915131493": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
